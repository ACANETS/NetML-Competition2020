{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Baseline Quickstart for NetML-Competition 2020\n",
    "\n",
    "### * Loads datasets, plots confusion matrix, prints evaluation metrics on validation set and create submission JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import time as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from utils.helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create submissions\n",
    "def do_submit(clf, test_set, scaler, class_label_pair, filepath):\n",
    "    Xtest, ids = get_submission_data(test_set)\n",
    "    X_test_scaled = scaler.transform(Xtest)\n",
    "    print(\"Predicting on {} ...\".format(test_set.split('/')[-1]))\n",
    "    predictions = clf.predict(X_test_scaled)\n",
    "    make_submission(predictions, ids, class_label_pair, filepath)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify only this cell:\n",
    "# Note: anno = \"mid\" is valid ONLY with non-vpn2016 dataset\n",
    "###\n",
    "dataset = \"./data/NetML\" # or \"./data/CICIDS2017\" or \"./data/non-vpn2016\"\n",
    "anno = \"top\" # or \"mid\" or \"fine\"\n",
    "submit = \"both\" # or \"test-std\" or \"test-challenge\"\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign variables\n",
    "training_set = dataset+\"/2_training_set\"\n",
    "training_anno_file = dataset+\"/2_training_annotations/2_training_anno_\"+anno+\".json.gz\"\n",
    "test_set = dataset+\"/1_test-std_set\"\n",
    "challenge_set = dataset+\"/0_test-challenge_set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder for the results\n",
    "time_ = t.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "save_dir = os.getcwd() + '/results/' + time_\n",
    "os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data in np.array format\n",
    "Xtrain, ytrain, class_label_pair, Xtrain_ids = get_training_data(training_set, training_anno_file)\n",
    "\n",
    "# Split validation set from training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(Xtrain, ytrain,\n",
    "                                                test_size=0.2, \n",
    "                                                random_state=42,\n",
    "                                                stratify=ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get name of each class to display in confusion matrix\n",
    "class_names = list(sorted(class_label_pair.keys()))\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RF Model\n",
    "print(\"Training the model ...\")\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs = -1, max_features=\"auto\")\n",
    "clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output accuracy of classifier\n",
    "print(\"Training Score: \\t{:.5f}\".format(clf.score(X_train_scaled, y_train)))\n",
    "print(\"Validation Score: \\t{:.5f}\".format(clf.score(X_val_scaled, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Confusion Matrix\n",
    "ypred = clf.predict(X_val_scaled)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(directory=save_dir, y_true=y_val, y_pred=ypred, \n",
    "                        classes=class_names, \n",
    "                        normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submission with JSON format\n",
    "if submit == \"test-std\" or submit == \"both\":\n",
    "    do_submit(clf, test_set, scaler, class_label_pair, save_dir+\"/submission_test-std.json\")\n",
    "if submit == \"test-challenge\" or submit == \"both\":\n",
    "    do_submit(clf, challenge_set, scaler, class_label_pair, save_dir+\"/submission_test-challenge.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
